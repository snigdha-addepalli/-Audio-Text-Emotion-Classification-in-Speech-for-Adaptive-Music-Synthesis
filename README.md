# Audio-Text Emotion Classification in Speech for Adaptive Music Synthesis

## Overview
This study focuses on developing a multimodal emotion classification system that detects emotions from audio and provides personalized music based on the identified emotional context. The system processes and augments audio data from standardized datasets (RAVDESS, CREMA, SAVEE, and TESS) to ensure robustness. Audio features such as Mel-Frequency Cepstral Coefficients (MFCCs) and spectral characteristics are extracted, high level audio features using wave2vec model, while text transcriptions, generated using OpenAIâ€™s Whisper model, are encoded with Word2Vec embeddings. Emotion classification is performed using a 7-layer Convolutional Neural Network (CNN), SVM and CNN-LSTM model. The integration of audio and text features improves the accuracy of emotion detection, which is then linked to dynamic music generation. This project offers a new framework for creating emotionally resonant content, with potential applications in media production, storytelling, and creative industries.

## Features

### Sentiment Analysis
- **Audio-Based Emotion Recognition**: Extracts key audio features such as MFCCs and spectral characteristics to detect emotions from speech.
- **Text-Based Emotion Recognition**: Utilizes text transcriptions generated by OpenAI's Whisper model and encodes them using Word2Vec embeddings for emotion detection.
- **Multimodal Integration**: Combines both audio and text-based emotion features to improve classification performance.
- **Emotion Classification**: Classifies emotions into categories like happiness, sadness, anger, surprise, etc.

### Music Mapping
- **Emotion-Driven Music Synthesis**: Selects personalized music based on the detected emotional context.

## Document Overview

- **/datasets**: contains raw and combined datasets.
- **/feature_csv**: extracted features from audio & text. 
- **/models**: contains emotion classification models.
- **/plots**: configured visualization of our model's results.
- **/predicted_emotions**: contains CSVs for actual emotions & predicted emotions.
- **/transcriptions**: audio transcription using WhisperAI.
- **cnn7layer.py**: CNN 7-layered model with 94.25% accuracy. (Best model)
- **cnn_7layer_8layer_l2_lstm.py**: Experimented with 7-layered, 8-layered with L2 and LSTM models for improving accuracy.
- **cnn_feature.comb.py**: combining text and audio features.
- **data_preprocess_and_text_feature_extraction_svm.py**: text feature extraction. Tried simple SVM and best fit SVM for increasing accuracy.
- **music_gen.py**: converts the classified emotion to music.
- **svm.py**: Experimented SVM for emotion classification.
- **svm_savee.py**: SVM using Savee(smallest dataset) to check for accuracy.
- **wav2vec.py**: feature extractions using Wav2Vec model.

## Getting Started

### Prerequisites

Ensure you have the following installed to run the front end application:
1. Python 3.8+
2. CUDA-enabled GPU (recommended for text and audio feature extraction)
3. System Dependencies: \
   **macOS:**
    ```bash
   brew install portaudio ffmpeg
    ```
   **Windows:**
   1. Install FFmpeg: \
      Download FFmpeg from FFmpeg Official Website \
      Add FFmpeg to your system PATH 
   2. Install Microsoft Visual C++ Build Tools: \
      Download from Microsoft Visual Studio \
      During installation, select "Desktop development with C++"

### Installation
#### Only follow these steps if you wish to run the application locally. 

1. Clone the repository:
   ```bash
   git clone https://github.com/c-huitt/emotion_music_gen.git
   ```

2. Navigate to the project directory:
   ```bash
   cd emotion_music_gen
   ```

3. Install the required packages:
   ```bash
   pip install -r requirements.txt
   ```

4. Unzip the large .csv files in the datasets folder:
   ```bash
   gunzip *.gz
   ```

5. Run the music_gen.py file:
   ```bash
   python music_gen.py
   ```

## Contributing

Feel free to contribute. Please fork the repository and submit a pull request if you choose to do so.

## Acknowledgments

This project uses the following datasets:
- [DEAM](https://cvml.unige.ch/databases/DEAM/s) 
- [SAVEE](http://kahlan.eps.surrey.ac.uk/savee/)
- [RAVDESS](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio)
- [TESS](https://utoronto.scholaris.ca/collections/036db644-9790-4ed0-90cc-be1dfb8a4b66)
- [CREMA-D](https://www.kaggle.com/datasets/ejlok1/cremad)

## Resources
- [Whisper AI](https://openai.com/index/whisper/)
